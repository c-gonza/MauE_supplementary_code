{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00814288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import re\n",
    "import Bio\n",
    "from Bio import Entrez\n",
    "import sys, errno, re, json, ssl\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from time import sleep\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this notebook I am going to get the data used for the species count donut chart and the phylum cluster map in the \n",
    "supplementary.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d396317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is modified from my data fetcher to add scientific names to my csv columns\n",
    "\n",
    "tax_LOT = pd.read_pickle(r\"C:\\PATH\\ncbi_2025_taxonomy_table.pkl\")\n",
    "print(tax_LOT)\n",
    "email = 'example@gmail.com' \n",
    "\n",
    "\n",
    "def fetch_TAXID_data(tax_id, email):\n",
    "    classification = tax_id\n",
    "    Entrez.email = email  \n",
    "    retry_limit = 3  \n",
    "\n",
    "    attempts = 0\n",
    "    while attempts < retry_limit:\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"taxonomy\", id=str(classification), retmode=\"xml\")\n",
    "            records = Entrez.read(handle)\n",
    "            for rec in records:\n",
    "                NAME = rec.get(\"ScientificName\")\n",
    "            break  \n",
    "        except Exception as e:\n",
    "            print(f\"HTTP error: {str(e)} - Sleeping 10s and retrying for ID {classification}\")\n",
    "            sleep(10)\n",
    "            attempts += 1  \n",
    "        finally:\n",
    "            if 'handle' in locals() and handle:\n",
    "                handle.close()  \n",
    "    if attempts == retry_limit:\n",
    "        print(f\"Failed after {retry_limit} retries for ID {classification}\")\n",
    "\n",
    "    return NAME\n",
    "\n",
    "\n",
    "\n",
    "def fetch_LINEAGE_NAME_data(lineage_list, email):\n",
    "    Entrez.email = email \n",
    "    lineage_df = {}\n",
    "    \n",
    "    for classification in lineage_list:  # same as first instance\n",
    "        try:\n",
    "            NAME_series = tax_LOT[tax_LOT['tax_id'] == classification]['name_txt']\n",
    "            if not NAME_series.empty:\n",
    "                NAME = NAME_series.iloc[0]  \n",
    "                if NAME:\n",
    "                    lineage_df[NAME] = classification\n",
    "                    print(f'NAME: {NAME}, Tax ID: {classification}')\n",
    "                else:\n",
    "                    print(f'No rank available for Tax ID: {classification}')\n",
    "            else:\n",
    "                print(f'No entry found for Tax ID: {classification}, fetching from NCBI...')\n",
    "                NAME = fetch_TAXID_data(classification, email)\n",
    "                if NAME and NAME != \"No NAME available\":\n",
    "                    lineage_df[NAME] = classification\n",
    "                else:\n",
    "                    print(f'Failed to fetch data for Tax ID: {classification} from NCBI.')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing Tax ID {classification}: {str(e)}')\n",
    "\n",
    "    return lineage_df\n",
    "\n",
    "# testing everything still works (0 fails):\n",
    "list_tax = ['1', '0', '2624677', '1644055', '2157', '1236']\n",
    "email = 'your_email@example.com'  # Replace with your actual email\n",
    "print(fetch_LINEAGE_NAME_data(list_tax, email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the false positive filtered species data\n",
    "species_summary_df = pd.read_excel(r\"C:\\PATH\\5_Filtered_FBDS_IPRcounts_byspecies_MAUA_names.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_summary_df2 = species_summary_df.fillna(0)\n",
    "print(species_summary_df2)\n",
    "#species_summary_df2 = species_summary_df2.drop('IPR026259', axis = 1)\n",
    "#print(species_summary_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPR dataframe masks:\n",
    "#getting counts of IPR combos\n",
    "\n",
    "total_species = species_summary_df2['species'].count()\n",
    "\n",
    "IPR009908_only = (species_summary_df2['IPR009908'] > 0) & (species_summary_df2['IPR003752'] == 0) & (species_summary_df2['IPR012932'] == 0)\n",
    "\n",
    "IPR012932_only = (species_summary_df2['IPR009908'] == 0) & (species_summary_df2['IPR003752'] == 0) & (species_summary_df2['IPR012932'] > 0)\n",
    "\n",
    "IPR003752_only = (species_summary_df2['IPR009908'] == 0) & (species_summary_df2['IPR003752'] > 0) & (species_summary_df2['IPR012932'] == 0)\n",
    "\n",
    "IPR003752_IPR009908_only = (species_summary_df2['IPR009908'] > 0) & (species_summary_df2['IPR003752'] > 0) & (species_summary_df2['IPR012932'] == 0)\n",
    "\n",
    "IPR003752_IPR012932 = (species_summary_df2['IPR012932'] > 0) & (species_summary_df2['IPR003752'] > 0) & (species_summary_df2['IPR009908'] == 0)\n",
    "\n",
    "IPR009908_IPR012932_only = (species_summary_df2['IPR009908'] > 0) & (species_summary_df2['IPR012932'] > 0)  & (species_summary_df2['IPR003752'] == 0)\n",
    "\n",
    "all_three = (species_summary_df2['IPR009908'] > 0) & (species_summary_df2['IPR012932'] > 0) & (species_summary_df2['IPR003752'] > 0)\n",
    "none = (species_summary_df2['IPR009908'] == 0) & (species_summary_df2['IPR012932'] == 0) & (species_summary_df2['IPR003752'] == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d70ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_species)\n",
    "print(IPR009908_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(species_summary_df2.groupby('phylum')['species'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95f036",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(species_summary_df2)\n",
    "\n",
    "#some masking to get the groups\n",
    "phylum_total = species_summary_df2.groupby('phylum')['species'].nunique()\n",
    "phylum_ipr009908 = species_summary_df2.loc[IPR009908_only].groupby('phylum')['species'].nunique()\n",
    "phylum_ipr012932 = species_summary_df2.loc[IPR012932_only].groupby('phylum')['species'].nunique()\n",
    "phylum_ipr003752 = species_summary_df2.loc[IPR003752_only].groupby('phylum')['species'].nunique()\n",
    "\n",
    "\n",
    "phylum_all_three = species_summary_df2.loc[all_three].groupby('phylum')['species'].nunique()\n",
    "phylum_ipr009908_ipr012932 = species_summary_df2.loc[IPR009908_IPR012932_only].groupby('phylum')['species'].nunique()\n",
    "phylum_ipr012932_ipr003752 = species_summary_df2.loc[IPR003752_IPR012932].groupby('phylum')['species'].nunique()\n",
    "phylum_ipr003752_ipr009908 = species_summary_df2.loc[IPR003752_IPR009908_only].groupby('phylum')['species'].nunique()\n",
    "\n",
    "phylum_none = species_summary_df2.loc[none].groupby('phylum')['species'].nunique()\n",
    "\n",
    "\n",
    "#making a new df\n",
    "phylum_summary_df = pd.DataFrame({\n",
    "    'total_count': phylum_total,\n",
    "    'IPR009908_only': phylum_ipr009908,\n",
    "    'IPR012932_only': phylum_ipr012932,\n",
    "    'IPR003752_only': phylum_ipr003752,\n",
    "    'IPR012932_IPR009908_only': phylum_ipr009908_ipr012932,\n",
    "    'IPR003752__IPR012932_only': phylum_ipr012932_ipr003752,\n",
    "    'IPR009908_IPR003752_only': phylum_ipr003752_ipr009908,\n",
    "\n",
    "    'All_three': phylum_all_three,\n",
    "    'None': phylum_none\n",
    "  \n",
    "})\n",
    "\n",
    "\n",
    "phylum_summary_df.reset_index(inplace=True)\n",
    "phylum_summary_df2 = phylum_summary_df.fillna(0)\n",
    "\n",
    "#check it\n",
    "display(phylum_summary_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#header_df = list(phylum_summary_df2.keys())\n",
    "    \n",
    "#phylum_summary_df2.to_csv(path_or_buf=r\"C:\\PATH\\full_filtered_set_IPR_counts_per_phylum_only.csv\",sep=\",\",header = header_df)\n",
    "#phylum_summary_df2.to_excel(r\"C:\\PATH\\6_FBDS_phylum_species_counts_nomauG_data_with3qualitymarkers.xlsx\")\n",
    "\n",
    "#run this if your just loading in the phylum summary\n",
    "phylum_summary_df2 = pd.read_excel(r\"C:\\PATH\\6_FBDS_phylum_species_counts_nomauG_data_with3qualitymarkers.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eea047",
   "metadata": {},
   "outputs": [],
   "source": [
    "phylum_summary_df3 = phylum_summary_df2.copy()\n",
    "IPR009908_only_list =[]\n",
    "IPR003752_only_list=[]\n",
    "IPR012932_only_list=[]\n",
    "IPR009908_IPR003752_only_list=[]\n",
    "IPR012932_IPR003752_only_list=[]\n",
    "IPR012932_IPR009908_only_list=[]\n",
    "all_three=[]\n",
    "none=[]\n",
    "\n",
    "for ind in phylum_summary_df3.index:\n",
    "    percent_IPR009908 = (phylum_summary_df3['IPR009908_only'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    percent_IPR003752 = (phylum_summary_df3['IPR003752_only'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    percent_IPR012932 = (phylum_summary_df3['IPR012932_only'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    percent_IPR012932_IPR003752 = (phylum_summary_df3['IPR003752__IPR012932_only'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    percent_IPR009908_IPR003752 = (phylum_summary_df3['IPR009908_IPR003752_only'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    percent_IPR009908_IPR012932 = (phylum_summary_df3['IPR012932_IPR009908_only'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    percent_all_three = (phylum_summary_df3['All_three'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    percent_none = (phylum_summary_df3['None'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "\n",
    "    IPR009908_only_list.append(percent_IPR009908)\n",
    "    IPR003752_only_list.append(percent_IPR003752)\n",
    "    IPR012932_only_list.append(percent_IPR012932)\n",
    "    IPR009908_IPR003752_only_list.append(percent_IPR009908_IPR003752)\n",
    "    IPR012932_IPR003752_only_list.append(percent_IPR012932_IPR003752)\n",
    "    IPR012932_IPR009908_only_list.append(percent_IPR009908_IPR012932)\n",
    "    all_three.append(percent_all_three)\n",
    "    none.append(percent_none)\n",
    "print(none)  \n",
    "phylum_summary_df3['perc_IPR009908'] = IPR009908_only_list\n",
    "phylum_summary_df3['perc_IPR012932']= IPR012932_only_list\n",
    "phylum_summary_df3['perc_IPR003752']= IPR003752_only_list\n",
    "phylum_summary_df3['perc_IPR009908_IPR003752']= IPR009908_IPR003752_only_list\n",
    "phylum_summary_df3['perc_IPR012932_IPR003752']= IPR012932_IPR003752_only_list\n",
    "phylum_summary_df3['perc_IPR012932_IPR009908']= IPR012932_IPR009908_only_list\n",
    "phylum_summary_df3['perc_all_three']= all_three\n",
    "phylum_summary_df3['perc_none']= none\n",
    "display(phylum_summary_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2fb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section is prone to failing because of NCBI http errors. so I doubled it up and put try and except loops\n",
    "#just manually fill any gaps left over. for my data set it pulled all 167 rows but it took a while. \n",
    "#it just adds the scientific names instead of id\n",
    "phylum_summary_df4 = phylum_summary_df3\n",
    "names = []\n",
    "for ind in phylum_summary_df4.index:\n",
    "    if phylum_summary_df4['phylum'][ind] == 0:\n",
    "        name = 'no_phylum'\n",
    "        names.append(name)\n",
    "    else:\n",
    "        try:\n",
    "            tax_id = phylum_summary_df4['phylum'][ind]\n",
    "            list_ind = [int(tax_id)] #I made the function accept a list which was good for the Interpro pull but annoying here \n",
    "            print(list_ind)\n",
    "            name_df = fetch_LINEAGE_NAME_data(list_ind,email)\n",
    "            temp_name_ls = list(name_df.keys())\n",
    "            print(temp_name_ls[0])\n",
    "            names.append(temp_name_ls[0])\n",
    "        except:\n",
    "            try:\n",
    "                tax_id = phylum_summary_df4['phylum'][ind]\n",
    "                list_ind = [int(tax_id)] #I made the function accept a list which was good for the Interpro pull but annoying here \n",
    "                print(list_ind)\n",
    "                name_df = fetch_LINEAGE_NAME_data(list_ind,email)\n",
    "                temp_name_ls = list(name_df.keys())\n",
    "                print(temp_name_ls[0])\n",
    "                names.append(temp_name_ls[0])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "                \n",
    "print(names)       \n",
    "#phylum_summary_df4['scientific_names'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7762d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small check\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46712ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding in the names because I lost them at the phylum condensing step\n",
    "phylum_summary_df4['scientific_names']=names\n",
    "display(phylum_summary_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0621ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting phylum data uncomment lines if you want csv\n",
    "#header_df = list(phylum_summary_df4.keys())\n",
    "#phylum_summary_df4.to_csv(path_or_buf=r\"C:\\PATH\\filtered_IPR_counts_percentages_per_phylum_NO_phy_removed.csv\",sep=\",\",header = header_df)\n",
    "\n",
    "#phylum_summary_df4.to_excel(r\"C:\\PATH\\7_phylum_iprs_with_perc_and_names_no_maug_three_quality_markers.xlsx\")\n",
    "\n",
    "#read excel if you are starting here\n",
    "phylum_summary_df4 = pd.read_excel(r\"C:\\PATH\\7_phylum_iprs_with_perc_and_names_no_maug_three_quality_markers.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df with only percentages\n",
    "phylum_percentages=phylum_summary_df4[['perc_IPR009908', 'perc_IPR012932', 'perc_IPR003752', 'perc_IPR009908_IPR003752', 'perc_IPR012932_IPR003752', 'perc_IPR012932_IPR009908', 'perc_all_three','perc_none', 'scientific_names']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16707b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removing candidatus phylums\n",
    "phylum_no_candidatus = phylum_summary_df4.copy()\n",
    "# Remove rows containing 'candidatus' in 'scientific_names'\n",
    "phylum_no_candidatus = phylum_no_candidatus[~phylum_no_candidatus['scientific_names'].str.contains('candidatus', case=False, na=False)]\n",
    "display(phylum_no_candidatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09385506",
   "metadata": {},
   "outputs": [],
   "source": [
    "phylum_percentages2=phylum_no_candidatus[['perc_IPR009908', 'perc_IPR012932', 'perc_IPR003752', 'perc_IPR009908_IPR003752', 'perc_IPR012932_IPR003752', 'perc_IPR012932_IPR009908', 'perc_all_three','perc_none', 'scientific_names']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda78869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing candidatus and no phylum or candidate so its just the concrete phylum. Some still remain after the first removal\n",
    "phylum_no_candidatus2 = phylum_no_candidatus[~phylum_no_candidatus['scientific_names'].str.contains('candidatus', case=False, na=False)]\n",
    "phylum_no_candidatus3 = phylum_no_candidatus2[~phylum_no_candidatus2['scientific_names'].str.contains('no_phylum', case=False, na=False)]\n",
    "phylum_no_candidatus4 = phylum_no_candidatus3[~phylum_no_candidatus3['scientific_names'].str.contains('candidate', case=False, na=False)]\n",
    "phylum_no_candidatus4.set_index('scientific_names', inplace=True)\n",
    "phylum_no_candidatus5 = phylum_no_candidatus4.copy()\n",
    "phylum_no_candidatus6 = phylum_no_candidatus5[\n",
    "    (phylum_no_candidatus5['total_count'] >= 10)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if your running the first time then export the data set otherwise import the data set and set the index to scientific names \n",
    "so you can reproduce the clustermap figure and the donut chart\n",
    "\n",
    "'''\n",
    "#phylum_no_candidatus6.to_excel(\"C:\\PATH\\8_phylum_percentages_cand_removed_no_maug_three_quality_markers.xlsx\")\n",
    "#read if starting here, reset index too\n",
    "phylum_no_candidatus6 = pd.read_excel(r\"C:\\PATH\\8_phylum_percentages_cand_removed_no_maug_three_quality_markers.xlsx\")\n",
    "phylum_no_candidatus6 = phylum_no_candidatus6.set_index('scientific_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(phylum_no_candidatus6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9f210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#making the clustermap\n",
    "# modify this to take or leave any percentage column\n",
    "phylum_no_candidatus7=phylum_no_candidatus6[['perc_IPR009908', 'perc_IPR012932', 'perc_IPR003752', 'perc_IPR009908_IPR003752', 'perc_IPR012932_IPR003752', 'perc_IPR012932_IPR009908', 'perc_all_three' ]].copy()\n",
    "g = sns.clustermap(phylum_no_candidatus7,figsize=(15,15),cmap='rocket')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.savefig(r\"C:\\PATH\\red_IPRS_perc_noMaug_no_none_clustermap_3qualitymarkers.png\", dpi='figure',  bbox_inches='tight' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e764cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phylum_no_candidatus6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the donut chart\n",
    "phylum_no_candidatus6 = pd.read_excel(r\"C:\\PATH\\8_phylum_percentages_cand_removed_no_maug_three_quality_markers.xlsx\")\n",
    "\n",
    "# summing for the donut\n",
    "maue = int(phylum_no_candidatus6['IPR009908_only'].sum()) + int(phylum_no_candidatus6['IPR012932_IPR009908_only'].sum()) + int(phylum_no_candidatus6['IPR009908_IPR003752_only'].sum()) + int(phylum_no_candidatus6['All_three'].sum())\n",
    "vkor = int(phylum_no_candidatus6['IPR012932_only'].sum()) + int(phylum_no_candidatus6['IPR012932_IPR009908_only'].sum()) + int(phylum_no_candidatus6['IPR003752__IPR012932_only'].sum()) + int(phylum_no_candidatus6['All_three'].sum())\n",
    "dsbb = int(phylum_no_candidatus6['IPR003752_only'].sum()) + int(phylum_no_candidatus6['IPR003752__IPR012932_only'].sum()) + int(phylum_no_candidatus6['IPR009908_IPR003752_only'].sum()) + int(phylum_no_candidatus6['All_three'].sum())\n",
    "none = int(phylum_no_candidatus6['None'].sum())\n",
    "total = int(phylum_no_candidatus6['total_count'].sum())\n",
    "\n",
    "# Data and labels\n",
    "data2 = [maue, dsbb, vkor, none]\n",
    "print(data2,total)\n",
    "names = ['MauE', 'DsbB', 'VKOR', 'None']\n",
    "print(data2)\n",
    "\n",
    "# Explosion and colors, for some reason the diameters are a little off so I tweeked them to be roughly same\n",
    "explode = (0.05,0,0.05,0)\n",
    "colors = sns.color_palette(\"flare\",n_colors=8)\n",
    "\n",
    "# this is a dumb way to assign the labels based on a function that determines whether the label matches the count \n",
    "#but I didnt want to start from scratch so here we are\n",
    "def func(pct, all_values):\n",
    "    total = sum(all_values)\n",
    "    absolute = int(round(pct * total / 100.0))\n",
    "    for i, value in enumerate(all_values):\n",
    "        if value == absolute:\n",
    "            return f\"{value}\"\n",
    "    return \"\"\n",
    "\n",
    "# Plotting \n",
    "plt.figure(figsize=(12, 12))  # Increase the chart size\n",
    "plt.pie(data2, colors=colors, autopct=lambda pct: func(pct, data2),\n",
    "        pctdistance=0.8, labels=names, explode=explode, textprops={'fontsize': 10, 'color': 'black'})  # Set label font size here\n",
    "\n",
    "# Add center circle for the vibes\n",
    "centre_circle = plt.Circle((0, 0), 0.50, fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(r'C:\\PATH\\LIGHTFLARE_IPRS_donut_chart3markers.png', dpi='figure',  bbox_inches='tight', transparent=True )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
