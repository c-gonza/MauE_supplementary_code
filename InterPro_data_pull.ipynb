{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2439b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the import section\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import sys, errno, re, json, ssl\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from time import sleep\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9762e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "the below functions pulls a list of strings that contain 2 lines;\n",
    "1.>fasta metadata as the description\n",
    "2.the amino acid sequence of the protein in question\n",
    "it requires a URL that will be shared for all these functions from the interpro API with the fasta additional information \n",
    "selected.\n",
    "\n",
    "There is also a bit of code for creating a taxanomic look up table from the latest taxanomic codes and their scientific names\n",
    "in the NCBI taxanomic database. The look up table gets saved as a pickle file. This was done to decrease the time needed to \n",
    "pull metadata on taxonomy for the InterPro entries by downloading them ahead of time and searching for information in the look \n",
    "up table. When the entries are not in the look up tabel instead the information is pulled by the Entrez system from NCBI through\n",
    "biopython. You need to download the zip files from the NCBI site to run the first time: https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/\n",
    "Then extract them to a folder to run the code for the lookup table.\n",
    "\n",
    "The API pull code initially was partially based on the guides provided by the InterPro API webpage but was modified.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DONT RUN UNLESS YOUR REMAKING THE LOOKUP TABLE###\n",
    "#how the taxonomy look up table was constructed:\n",
    "\n",
    "taxfilepath = 'YOUR/PATH/HERE/new_taxdump'  # Update this to your actual path for taxdump\n",
    "\n",
    "#col names in the nodes dmp file, for parsing the dmp file\n",
    "nodes_cols = ['tax_id', 'parent_tax_id', 'rank', 'embl_code', 'division_id', 'inherited_div_flag',\n",
    "              'genetic_code_id', 'inherited_GC_flag', 'mitochondrial_genetic_code_id', 'inherited_MGC_flag',\n",
    "              'GenBank_hidden_flag', 'hidden_subtree_root_flag', 'comments','plastid genetic code id','inherited PGC flag',\n",
    "              'specified_species',' hydrogenosome genetic code id',' inherited HGC flag ']\n",
    "nodes = pd.read_csv(f'{taxfilepath}/nodes.dmp', sep='\\t\\|\\t', engine='python', names=nodes_cols, usecols=['tax_id', 'parent_tax_id', 'rank'], dtype=str)\n",
    "\n",
    "nodes['tax_id'] = nodes['tax_id'].str.strip()\n",
    "nodes['parent_tax_id'] = nodes['parent_tax_id'].str.strip()\n",
    "nodes['rank'] = nodes['rank'].str.strip()\n",
    "print(nodes)\n",
    "\n",
    "\n",
    "names_cols = ['tax_id', 'name_txt', 'unique_name', 'name_class']\n",
    "names = pd.read_csv(f'{taxfilepath}/names.dmp', sep='\\t\\|\\t', engine='python', names=names_cols, usecols=['tax_id', 'name_txt', 'name_class'], dtype=str)\n",
    "names['tax_id'] = names['tax_id'].str.strip()\n",
    "names['name_txt'] = names['name_txt'].str.strip()\n",
    "names['name_class'] = names['name_class'].str.strip()\n",
    "print(names)\n",
    "\n",
    "scientific_names = names[names['name_class'] == 'scientific name\\t|']\n",
    "\n",
    "\n",
    "taxonomy = pd.merge(nodes, scientific_names, on='tax_id')\n",
    "\n",
    "#this is a test to make sure the table works before making the pkl\n",
    "print(taxonomy)\n",
    "print(taxonomy[taxonomy['tax_id'] == '9606'])  # for human\n",
    "print(taxonomy[taxonomy['tax_id'] == '9606']['rank'])\n",
    "\n",
    "taxonomy.to_pickle('YOUR/PATH/HERE/new_taxdump/ncbi_2024_taxonomy_table.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca33fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function just pulls fastas and metadata saving things seperatly as individual fasta files\n",
    "\n",
    "import json\n",
    "import ssl\n",
    "from urllib import request, error\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "def output_fasta_list(base_url):\n",
    "    context = ssl._create_unverified_context()\n",
    "    next_url = base_url\n",
    "    FASTA_STRINGS = [] \n",
    "\n",
    "    while next_url:\n",
    "        HEADER_SEPARATOR = \"|\"\n",
    "        try:\n",
    "            req = request.Request(next_url, headers={\"Accept\": \"application/json\"})\n",
    "            with request.urlopen(req, context=context) as res:\n",
    "                if res.status == 408:  \n",
    "                    sleep(61)\n",
    "                    continue\n",
    "                elif res.status == 204:  \n",
    "                    break\n",
    "                payload = json.loads(res.read().decode())\n",
    "                next_url = payload.get(\"next\") \n",
    "\n",
    "            for item in payload.get(\"results\", []):\n",
    "                entries = item.get(\"entry_subset\") or item.get(\"entries\", [])\n",
    "                taxa = item.get(\"taxa\", [{\"lineage\": []}])\n",
    "                taxa_header = \"|\".join(taxa[0].get(\"lineage\", []))\n",
    "                #this part is modified to so the fasta files includes the metadata for down stream stuff I did. sperated by |\n",
    "                if entries:\n",
    "                    fasta_header = (\n",
    "                        \">\" + item[\"metadata\"].get(\"accession\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        entries[0].get(\"accession\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        item[\"metadata\"].get(\"name\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        item[\"metadata\"][\"source_organism\"].get(\"taxId\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        item[\"metadata\"][\"source_organism\"].get(\"scientificName\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        taxa_header\n",
    "                    )\n",
    "                else:\n",
    "                    fasta_header = \">\" + item[\"metadata\"].get(\"accession\", \"unknown\") + HEADER_SEPARATOR + item[\"metadata\"].get(\"name\", \"unknown\")\n",
    "\n",
    "                fasta_seq = item.get(\"extra_fields\", {}).get(\"sequence\", \"\")\n",
    "                fasta_string = fasta_header + \"\\n\" + fasta_seq\n",
    "                FASTA_STRINGS.append(fasta_string)\n",
    "\n",
    "        except error.HTTPError as e:\n",
    "            sys.stderr.write(\"Error processing URL: \" + str(next_url) + \"; Error: \" + str(e) + \"\\n\")\n",
    "            if e.code == 408 and attempts < 3:\n",
    "                attempts += 1\n",
    "                sleep(61)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        sleep(1)  # Rate limit delay\n",
    "\n",
    "    return FASTA_STRINGS  # Return after fully processing all pages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function exports the individual fasta files to a given path\n",
    "#if you are using windows you must changes the \\ in the path to / or us the r''\n",
    "\n",
    "def fasta_file_write(output_fasta_list,path):\n",
    "    fasta_list = output_fasta_list\n",
    "    pathvar = path\n",
    "    \n",
    "    for fasta in fasta_list:\n",
    "        fasta_string = fasta\n",
    "        fasta_string_2 = fasta\n",
    "        fasta_string_name , fasta_string_seq = fasta_string.strip('>').split('\\n')\n",
    "        print(fasta_string_name,fasta_string_seq,'\\n')\n",
    "        filename_list = fasta_string_2.strip('>').split('|')\n",
    "        filename = filename_list[0]\n",
    "        rec = SeqRecord(\n",
    "            Seq(fasta_string_seq),\n",
    "            id='',\n",
    "            name='',\n",
    "            description = fasta_string_name)\n",
    "        print('SeqRecord object : ',rec,sep='\\n')\n",
    "        records = [rec]\n",
    "        SeqIO.write(records,'{}/{}.fasta'.format(pathvar,filename),'fasta')\n",
    "        print('done with : ','{}/{}.fasta'.format(pathvar,filename))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7118eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I load the NCBI pkl file after its made. Its not a bad idea to clear your variable at this point or restart your kernal for mem\n",
    "tax_LOT = pd.read_pickle(\"YOUR/PATH/HERE/new_taxdump/ncbi_2024_taxonomy_table.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the NCBI taxanomic ID handling for getting IDs and ranks of the IDs so all entries get lineage data\n",
    "#these are called below\n",
    "def fetch_TAXID_data(tax_id, email):\n",
    "    classification = tax_id\n",
    "    Entrez.email = email  # Set the email directly here\n",
    "    retry_limit = 3  # Maximum number of retries for the ENTREZ PULL i set this to be 3 for speed\n",
    "\n",
    "    attempts = 0\n",
    "    while attempts < retry_limit:\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"taxonomy\", id=str(classification), retmode=\"xml\")\n",
    "            records = Entrez.read(handle)\n",
    "            for rec in records:\n",
    "                rank = rec.get(\"Rank\")\n",
    "            break  \n",
    "        except Exception as e:\n",
    "            print(f\"HTTP error: {str(e)} - Sleeping 10s and retrying for ID {classification}\")\n",
    "            sleep(10)\n",
    "            attempts += 1  # Increment attempts\n",
    "        finally:\n",
    "            if 'handle' in locals() and handle:\n",
    "                handle.close()  \n",
    "    if attempts == retry_limit:\n",
    "        print(f\"Failed after {retry_limit} retries for ID {classification}\")\n",
    "\n",
    "    return rank\n",
    "\n",
    "\n",
    "\n",
    "def fetch_LINEAGE_data(lineage_list, email):\n",
    "    Entrez.email = email  \n",
    "    lineage_df = {}\n",
    "    \n",
    "    for classification in lineage_list[2:]:  # Assuming the first two entries are to be skipped, this is root and cell orgs and skips to bacter (id:2)\n",
    "        try:\n",
    "            rank_series = tax_LOT[tax_LOT['tax_id'] == classification]['rank']\n",
    "            if not rank_series.empty:\n",
    "                rank = rank_series.iloc[0]  \n",
    "                if rank:\n",
    "                    lineage_df[rank] = classification\n",
    "                    print(f'Rank: {rank}, Tax ID: {classification}')\n",
    "                else:\n",
    "                    print(f'No rank available for Tax ID: {classification}')\n",
    "            else:\n",
    "                print(f'No entry found for Tax ID: {classification}, fetching from NCBI...')\n",
    "                rank = fetch_TAXID_data(classification, email)\n",
    "                if rank and rank != \"No rank available\":\n",
    "                    lineage_df[rank] = classification\n",
    "                else:\n",
    "                    print(f'Failed to fetch data for Tax ID: {classification} from NCBI.')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing Tax ID {classification}: {str(e)}')\n",
    "\n",
    "    return lineage_df\n",
    "\n",
    "# testing everything still works:\n",
    "list_tax = ['1', '0', '2624677', '1644055', '2157', '1236']\n",
    "email = 'your_email@example.com'  # Replace with your actual email\n",
    "print(fetch_LINEAGE_data(list_tax, email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e450fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this function creates a list of dictionaries. The dictionaries are Key value pairs:\n",
    "\"accession\" : A0X009XJT10 , \"IPR\" : IPR009908 , etc,....\n",
    "I have set it up so that it pulls what I consider to be all the useful information contained in the requested JSON\n",
    "dictionary from the interpro pull\n",
    "if you want to modify it open the url in the browser to see the actual JSON contents or print them with your own code\n",
    "then odify the for loop that contains the metadata section (its around \"metadata = {'Accession': accession,.....\")\n",
    "or look at InterPro API guide\n",
    "\n",
    "'''\n",
    "def fetch_data(BASE_URL,email):\n",
    "    context = ssl._create_unverified_context()\n",
    "    next_url = BASE_URL\n",
    "    data_list = []  # List to hold each entry's data\n",
    "    email = email\n",
    "\n",
    "    while next_url:\n",
    "        try:\n",
    "            req = request.Request(next_url, headers={\"Accept\": \"application/json\"})\n",
    "            res = request.urlopen(req, context=context)\n",
    "\n",
    "            if res.status == 408:\n",
    "                sleep(61)\n",
    "                continue\n",
    "            elif res.status == 204:\n",
    "                break\n",
    "\n",
    "            payload = json.loads(res.read().decode())\n",
    "            next_url = payload.get(\"next\", None)\n",
    "            #check that the json isnt empty\n",
    "            for i, item in enumerate(payload[\"results\"]):\n",
    "                entries = None\n",
    "                if (\"entry_subset\" in item):\n",
    "                    entries = item[\"entry_subset\"]\n",
    "                elif (\"entries\" in item):\n",
    "                    entries = item[\"entries\"]\n",
    "                    \n",
    "            #parsing the JSON from the results of the API pull\n",
    "            for item in payload[\"results\"]:\n",
    "                taxa = item['taxa']\n",
    "                accession = item[\"metadata\"][\"accession\"]\n",
    "                ipr = \"{}\".format(entries[0][\"accession\"]) \n",
    "                name = item[\"metadata\"][\"name\"]\n",
    "                gene = item[\"metadata\"][\"gene\"]\n",
    "                protein_length = \"{}\".format(entries[0][\"protein_length\"])\n",
    "                in_alphafold = \"{}\".format(item[\"metadata\"][\"in_alphafold\"])\n",
    "                source_org_ID = item[\"metadata\"][\"source_organism\"][\"taxId\"]\n",
    "                source_org_name = item[\"metadata\"][\"source_organism\"][\"scientificName\"]\n",
    "                taxa_range = len(taxa[0][\"lineage\"])\n",
    "                sequence = item[\"extra_fields\"][\"sequence\"]\n",
    "                \n",
    "                #this is the metadata used to construct the full dataset used in the publication for every interpro IPR\n",
    "                metadata = {'Accession': accession,\n",
    "                                  'IPR': ipr, \n",
    "                                  'name': name,\n",
    "                                  'gene': gene,\n",
    "                                  'protein_length': protein_length,\n",
    "                                  'in_alphafold': in_alphafold,\n",
    "                                  'Source_Org_ID': source_org_ID,\n",
    "                                  'Source_Org_Name': source_org_name,\n",
    "                                  'taxa_range' : taxa_range,\n",
    "                                  'Sequence': sequence,\n",
    "                                 }\n",
    "               \n",
    "                lineage_df = fetch_LINEAGE_data(taxa[0]['lineage'],email)\n",
    "                metadata.update(lineage_df)\n",
    "                data_list.append(metadata)\n",
    "                \n",
    "            \n",
    "\n",
    "            sleep(1)  \n",
    "\n",
    "        except HTTPError as e:\n",
    "            if e.code == 408:\n",
    "                sleep(61)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                break\n",
    "\n",
    "    return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this simple function just creates a dataframe from the fetch_data list \n",
    "\n",
    "def create_dataframe(data_list):\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "# this function creates a csv file for exporting the dataframe, you dont need to use this if you dont to\n",
    "#I just used this for personal ease of use later whe merging data from different API pulls.\n",
    "\n",
    "def construct_csv(dataframe,pathvar,filename):\n",
    "    path = pathvar\n",
    "    name = filename\n",
    "    working_df = dataframe\n",
    "    header_df = list(working_df.keys())\n",
    "    \n",
    "    working_df.to_csv(path_or_buf=\"{}/{}.csv\".format(path,name),sep=\",\",header = header_df)\n",
    "    print(\"exported : \" + name + \" to : \" + path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294248c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#code used just for fastas this DOES NOT construct the data set\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "  #this url is provided by InterPro\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR012932/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# fasta stuff\n",
    "  email = 'your@email.com'  \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "\n",
    "  print(\"done VKOR\")\n",
    "    \n",
    "########################\n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR009908/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# fasta stuff\n",
    "  email = 'your@email.com'\n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "\n",
    "  print(\"done MauE\")\n",
    "    \n",
    "########################\n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR003752/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# fasta stuff\n",
    "  email = 'your@email.com'  \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "\n",
    "  print(\"done DsbB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25831f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPRs used: \n",
    "#    MauE: IPR009908\n",
    "#    VKOR: IPR012932\n",
    "#    DsbB: IPR003752\n",
    "#URLS used for bacteria from interpro:\n",
    "#    VKOR: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR012932/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "#    MauE: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR009908/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "#    DsbB: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR003752/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code used for main data acquisition of the complete data set, it was done in multiple parts and takes a few days for all the data\n",
    "#can be decreased by using thread pools.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR012932/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "  email = 'your@email.com'   \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "#exporting dataset to csv\n",
    "\n",
    "  archae_list = fetch_data(url,email)\n",
    "  archaea_df = create_dataframe(archaea_list)\n",
    "  name = \"bacteria_vkor_set\"\n",
    "  construct_csv(archaea_df,pathvar,name)\n",
    "\n",
    "  print(\"done VKOR\")\n",
    "    \n",
    "########################\n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR009908/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "  email = 'your@email.com' \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "#exporting dataset to csv\n",
    "\n",
    "  archaea_list = fetch_data(url,email)\n",
    "  archaea_df = create_dataframe(archaea_list)\n",
    "  name = \"bacteria_maue_set\"\n",
    "  construct_csv(archaea_df,pathvar,name)\n",
    "\n",
    "  print(\"done MauE\")\n",
    "    \n",
    "########################\n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR003752/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "  email = 'your@email.com'  \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "#exporting dataset to csv\n",
    "\n",
    "  archaea_list = fetch_data(url,email)\n",
    "  archaea_df = create_dataframe(archaea_list)\n",
    "  name = \"bacteria_dsbb_set\"\n",
    "  construct_csv(archaea_df,pathvar,name)\n",
    "\n",
    "  print(\"done DsbB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71005e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this sesction is for the control set it includes IPR\n",
    "# for ribsosomal_sub IPR047873: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR047873/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "# for RNA polymeras_sub IPR010243:  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR010243/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "# for signal peptidase IPR036286: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR036286/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# signal peptidase has far more entries and takes the longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code used for main data acquisition\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR047873/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "\n",
    "#exporting dataset to csv\n",
    "\n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_RIBOSOME_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done Ribosomal subunit\")\n",
    "    \n",
    "########################\n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR010243/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "\n",
    "#exporting dataset to csv\n",
    "\n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_RIBOSOME_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done RNA poly subunit\")\n",
    "    \n",
    "########################\n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR036286/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "\n",
    "#exporting dataset to csv\n",
    "  \n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_RIBOSOME_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done singnal peptidase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc569a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section is for MauG to determine its overlap with MauE\n",
    "#https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR026259/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR026259/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "\n",
    "#exporting dataset to csv\n",
    "\n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_MAUG_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done with MauG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec769433",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this below section is for the MauA data and used IPR036560\n",
    "### the species is tagged at the end and the entire thing is a for loop which cuts down the results from 1.2M to like less than 100K hopefully :D\n",
    "#https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR036560/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = 'YOUR/PATH/HERE'\n",
    "\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR036560/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "\n",
    "#exporting dataset to csv\n",
    "\n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_MAUA_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done with MauA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data was then merged used as the foundation of the analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
