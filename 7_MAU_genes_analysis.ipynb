{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2667669",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This remakes the clustered heatmap for the Mau genes. the catagories are by the presence of MauA, MauE and MauG regardless\n",
    "overlap i.e if a species has more MauA and MauE then its counted as 1 for both. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ad011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import re\n",
    "import Bio\n",
    "from Bio import Entrez\n",
    "import sys, errno, re, json, ssl\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from time import sleep\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f96629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is modified from my data fetcher to add scientific names to my csv columns\n",
    "tax_LOT = pd.read_pickle(\"C:\\PATH\\ncbi_2025_taxonomy_table.pkl\")\n",
    "print(tax_LOT)\n",
    "email = 'example@gmail.com' \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_TAXID_data(tax_id, email):\n",
    "    classification = tax_id\n",
    "    Entrez.email = email \n",
    "    retry_limit = 3  \n",
    "\n",
    "    attempts = 0\n",
    "    while attempts < retry_limit:\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"taxonomy\", id=str(classification), retmode=\"xml\")\n",
    "            records = Entrez.read(handle)\n",
    "            for rec in records:\n",
    "                NAME = rec.get(\"ScientificName\")\n",
    "            break \n",
    "        except Exception as e:\n",
    "            print(f\"HTTP error: {str(e)} - Sleeping 10s and retrying for ID {classification}\")\n",
    "            sleep(10)\n",
    "            attempts += 1  \n",
    "        finally:\n",
    "            if 'handle' in locals() and handle:\n",
    "                handle.close() \n",
    "    if attempts == retry_limit:\n",
    "        print(f\"Failed after {retry_limit} retries for ID {classification}\")\n",
    "\n",
    "    return NAME\n",
    "\n",
    "\n",
    "\n",
    "def fetch_LINEAGE_NAME_data(lineage_list, email):\n",
    "    Entrez.email = email  \n",
    "    lineage_df = {}\n",
    "    \n",
    "    for classification in lineage_list:  # same as first\n",
    "        try:\n",
    "            NAME_series = tax_LOT[tax_LOT['tax_id'] == classification]['name_txt']\n",
    "            if not NAME_series.empty:\n",
    "                NAME = NAME_series.iloc[0]\n",
    "                if NAME:\n",
    "                    lineage_df[NAME] = classification\n",
    "                    print(f'NAME: {NAME}, Tax ID: {classification}')\n",
    "                else:\n",
    "                    print(f'No rank available for Tax ID: {classification}')\n",
    "            else:\n",
    "                print(f'No entry found for Tax ID: {classification}, fetching from NCBI...')\n",
    "                NAME = fetch_TAXID_data(classification, email)\n",
    "                if NAME and NAME != \"No NAME available\":\n",
    "                    lineage_df[NAME] = classification\n",
    "                else:\n",
    "                    print(f'Failed to fetch data for Tax ID: {classification} from NCBI.')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing Tax ID {classification}: {str(e)}')\n",
    "\n",
    "    return lineage_df\n",
    "\n",
    "# testing everything still works (0 fails):\n",
    "list_tax = ['1', '0', '2624677', '1644055', '2157', '1236']\n",
    "email = 'your_email@example.com'  # Replace with your actual email\n",
    "print(fetch_LINEAGE_NAME_data(list_tax, email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the false positive filtered species data\n",
    "species_summary_df = pd.read_excel(r\"C:\\PATH\\5_Filtered_FBDS_IPRcounts_byspecies_MAUA_names.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c38808",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_summary_df2 = species_summary_df.fillna(0)\n",
    "print(species_summary_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91932e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#getting counts of IPR combos\n",
    "\n",
    "total_species = species_summary_df2['species'].count()\n",
    "\n",
    "IPR009908_containing = (species_summary_df2['IPR009908'] > 0) \n",
    "\n",
    "IPR026259_containing =(species_summary_df2['IPR026259'] > 0)\n",
    "\n",
    "IPR036560_containing = (species_summary_df2['IPR036560'] > 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(species_summary_df2)\n",
    "\n",
    "\n",
    "phylum_total = species_summary_df2.groupby('phylum')['species'].nunique()\n",
    "phylum_ipr009908 = species_summary_df2.loc[IPR009908_containing].groupby('phylum')['species'].nunique()\n",
    "phylum_ipr026259 = species_summary_df2.loc[IPR026259_containing].groupby('phylum')['species'].nunique()\n",
    "phylum_ipr036560 = species_summary_df2.loc[IPR036560_containing].groupby('phylum')['species'].nunique()\n",
    "\n",
    "\n",
    "# Create a new DataFrame \n",
    "phylum_summary_df = pd.DataFrame({\n",
    "    'total_count': phylum_total,\n",
    "    'IPR009908': phylum_ipr009908,\n",
    "    'IPR026259': phylum_ipr026259,\n",
    "    'IPR036560': phylum_ipr036560\n",
    "})\n",
    "\n",
    "\n",
    "phylum_summary_df.reset_index(inplace=True)\n",
    "phylum_summary_df2 = phylum_summary_df.fillna(0)\n",
    "\n",
    "# Display the new DataFrame\n",
    "display(phylum_summary_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phylum_summary_df3 = phylum_summary_df2.copy()\n",
    "IPR009908_list =[]\n",
    "IPR036560_list=[]\n",
    "IPR026259_list=[]\n",
    "\n",
    "\n",
    "for ind in phylum_summary_df3.index:\n",
    "    percent_IPR009908 = (phylum_summary_df3['IPR009908'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    percent_IPR036560 = (phylum_summary_df3['IPR036560'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    percent_IPR026259 = (phylum_summary_df3['IPR026259'][ind]/phylum_summary_df3['total_count'][ind])*100\n",
    "    \n",
    "\n",
    "    IPR009908_list.append(percent_IPR009908)\n",
    "    IPR036560_list.append(percent_IPR036560)\n",
    "    IPR026259_list.append(percent_IPR026259)\n",
    "    \n",
    "\n",
    "phylum_summary_df3['perc_IPR009908'] = IPR009908_list\n",
    "phylum_summary_df3['perc_IPR026259']= IPR026259_list\n",
    "phylum_summary_df3['perc_IPR036560']= IPR036560_list\n",
    "\n",
    "display(phylum_summary_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a0a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section is prone to failing because of NCBI http errors. so I doubled it up and put try and except loops\n",
    "#just manually fill any gaps left over. for my data set it pulled all 167 rows but it took a while. \n",
    "#it just adds the scientific names instead of id\n",
    "phylum_summary_df4 = phylum_summary_df3\n",
    "names = []\n",
    "for ind in phylum_summary_df4.index:\n",
    "    if phylum_summary_df4['phylum'][ind] == 0:\n",
    "        name = 'no_phylum'\n",
    "        names.append(name)\n",
    "    else:\n",
    "        try:\n",
    "            tax_id = phylum_summary_df4['phylum'][ind]\n",
    "            list_ind = [int(tax_id)] #I made the function accept a list which was good for the Interpro pull but annoying here \n",
    "            print(list_ind)\n",
    "            name_df = fetch_LINEAGE_NAME_data(list_ind,email)\n",
    "            temp_name_ls = list(name_df.keys())\n",
    "            print(temp_name_ls[0])\n",
    "            names.append(temp_name_ls[0])\n",
    "        except:\n",
    "            try:\n",
    "                tax_id = phylum_summary_df4['phylum'][ind]\n",
    "                list_ind = [int(tax_id)] #I made the function accept a list which was good for the Interpro pull but annoying here \n",
    "                print(list_ind)\n",
    "                name_df = fetch_LINEAGE_NAME_data(list_ind,email)\n",
    "                temp_name_ls = list(name_df.keys())\n",
    "                print(temp_name_ls[0])\n",
    "                names.append(temp_name_ls[0])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "                \n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f17644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding in the names\n",
    "phylum_summary_df4['scientific_names']=names\n",
    "display(phylum_summary_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b652ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phylum_summary_df4.to_excel(\"C:\\PATH\\MAUgenes_3_cat_3markers_percents_names.xlsx\")\n",
    "phylum_summary_df4 = pd.read_excel(r\"C:\\PATH\\MAUgenes_3_cat_3markers_percents_names.xlsx\")\n",
    "phylum_summary_df4 = phylum_summary_df4.drop('Unnamed: 0', axis=1) #do this if your just loading the saved dataframe again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "phylum_percentages=phylum_summary_df4[['perc_IPR009908', 'perc_IPR026259', 'perc_IPR036560', 'scientific_names']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing candidatus phylums\n",
    "phylum_no_candidatus = phylum_summary_df4.copy()\n",
    "phylum_no_candidatus = phylum_no_candidatus[~phylum_no_candidatus['scientific_names'].str.contains('candidatus', case=False, na=False)]\n",
    "display(phylum_no_candidatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a80ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "phylum_percentages2=phylum_no_candidatus[['perc_IPR009908', 'perc_IPR026259', 'perc_IPR036560', 'scientific_names']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing candidatus and no phylum or candidate so its just he concrete phylum\n",
    "phylum_no_candidatus2 = phylum_no_candidatus[~phylum_no_candidatus['scientific_names'].str.contains('candidatus', case=False, na=False)]\n",
    "phylum_no_candidatus3 = phylum_no_candidatus2[~phylum_no_candidatus2['scientific_names'].str.contains('no_phylum', case=False, na=False)]\n",
    "phylum_no_candidatus4 = phylum_no_candidatus3[~phylum_no_candidatus3['scientific_names'].str.contains('candidate', case=False, na=False)]\n",
    "phylum_no_candidatus4.set_index('scientific_names', inplace=True)\n",
    "phylum_no_candidatus5 = phylum_no_candidatus4.copy()\n",
    "phylum_no_candidatus6 = phylum_no_candidatus5[\n",
    "    (phylum_no_candidatus5['total_count'] >= 10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phylum_no_candidatus6.to_excel(\"C:\\PATH\\MAUgenes_3cat_3mark_perc_no_can_10more_names.xlsx\")\n",
    "phylum_no_candidatus6 = pd.read_excel(r\"C:\\PATH\\MAUgenes_3cat_3mark_perc_no_can_10more_names.xlsx\")\n",
    "phylum_no_candidatus6 = phylum_no_candidatus6.set_index('scientific_names') #do this if your loading in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(phylum_no_candidatus6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf271e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "phylum_no_candidatus7=phylum_no_candidatus6[['perc_IPR036560','perc_IPR009908', 'perc_IPR026259' ]].copy()\n",
    "g = sns.clustermap(phylum_no_candidatus7,figsize=(7,15),cmap='rocket_r',col_cluster=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.savefig(r'C:\\PATH\\MauGenes_3.png', dpi='figure',  bbox_inches='tight' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40fd96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
