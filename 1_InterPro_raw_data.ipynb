{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these were run before 24.6.25 and reflect interpro data entries from this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2439b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import sys, errno, re, json, ssl\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from time import sleep\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9762e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the below function pulls a list of strings that contain 2 lines;\n",
    "#>fasta metadata as the description\n",
    "#the amino acid sequence of the protein in question\n",
    "#it requires a URL that will be shared for all these functions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DONT RUN unless your making the pkl file that has the taxonomy lookup table. \n",
    "#I made the look up table to try and make the metadata acquisition faster by pre downloading taxid dumps from ncbi\n",
    "#it only helps sometimes so its a minor speed boost. many times I still have to pull the taxid from the api \n",
    "\n",
    "#how the taxonomy look up table was constructed:\n",
    "\n",
    "taxfilepath = \"/PATH\" \n",
    "\n",
    "# Load nodes.dmp specifically for how the ncbi dmo looks\n",
    "nodes_cols = ['tax_id', 'parent_tax_id', 'rank', 'embl_code', 'division_id', 'inherited_div_flag',\n",
    "              'genetic_code_id', 'inherited_GC_flag', 'mitochondrial_genetic_code_id', 'inherited_MGC_flag',\n",
    "              'GenBank_hidden_flag', 'hidden_subtree_root_flag', 'comments','plastid genetic code id','inherited PGC flag',\n",
    "              'specified_species',' hydrogenosome genetic code id',' inherited HGC flag ']\n",
    "nodes = pd.read_csv(f'{taxfilepath}/nodes.dmp', sep='\\t\\|\\t', engine='python', names=nodes_cols, usecols=['tax_id', 'parent_tax_id', 'rank'], dtype=str)\n",
    "\n",
    "nodes['tax_id'] = nodes['tax_id'].str.strip()\n",
    "nodes['parent_tax_id'] = nodes['parent_tax_id'].str.strip()\n",
    "nodes['rank'] = nodes['rank'].str.strip()\n",
    "print(nodes)\n",
    "# Load names.dmp with the same considerations\n",
    "names_cols = ['tax_id', 'name_txt', 'unique_name', 'name_class']\n",
    "names = pd.read_csv(f'{taxfilepath}/names.dmp', sep='\\t\\|\\t', engine='python', names=names_cols, usecols=['tax_id', 'name_txt', 'name_class'], dtype=str)\n",
    "names['tax_id'] = names['tax_id'].str.strip()\n",
    "names['name_txt'] = names['name_txt'].str.strip()\n",
    "names['name_class'] = names['name_class'].str.strip()\n",
    "print(names)\n",
    "\n",
    "scientific_names = names[names['name_class'] == 'scientific name\\t|']\n",
    "\n",
    "# Merge dataframes\n",
    "taxonomy = pd.merge(nodes, scientific_names, on='tax_id')\n",
    "\n",
    "#testing the query\n",
    "print(taxonomy)\n",
    "print(taxonomy[taxonomy['tax_id'] == '9606'])  # Example using Human tax_id\n",
    "print(taxonomy[taxonomy['tax_id'] == '9606']['rank'])\n",
    "\n",
    "taxonomy.to_pickle(\"/PATH/ncbi_2025_taxonomy_table.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca33fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ssl\n",
    "from urllib import request, error\n",
    "from time import sleep\n",
    "import sys\n",
    "# this is basically all from the intepro API code so you can look there as well\n",
    "def output_fasta_list(base_url):\n",
    "    context = ssl._create_unverified_context()\n",
    "    next_url = base_url\n",
    "    FASTA_STRINGS = []  \n",
    "\n",
    "    while next_url:\n",
    "        HEADER_SEPARATOR = \"|\"\n",
    "        try:\n",
    "            req = request.Request(next_url, headers={\"Accept\": \"application/json\"})\n",
    "            with request.urlopen(req, context=context) as res:\n",
    "                if res.status == 408:  \n",
    "                    sleep(61)\n",
    "                    continue\n",
    "                elif res.status == 204:  \n",
    "                    break\n",
    "                payload = json.loads(res.read().decode())\n",
    "                next_url = payload.get(\"next\")  \n",
    "            for item in payload.get(\"results\", []):\n",
    "                entries = item.get(\"entry_subset\") or item.get(\"entries\", [])\n",
    "                taxa = item.get(\"taxa\", [{\"lineage\": []}])\n",
    "                taxa_header = \"|\".join(taxa[0].get(\"lineage\", []))\n",
    "\n",
    "                if entries:\n",
    "                    fasta_header = (\n",
    "                        \">\" + item[\"metadata\"].get(\"accession\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        entries[0].get(\"accession\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        item[\"metadata\"].get(\"name\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        item[\"metadata\"][\"source_organism\"].get(\"taxId\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        item[\"metadata\"][\"source_organism\"].get(\"scientificName\", \"unknown\") + HEADER_SEPARATOR +\n",
    "                        taxa_header\n",
    "                    )\n",
    "                else:\n",
    "                    fasta_header = \">\" + item[\"metadata\"].get(\"accession\", \"unknown\") + HEADER_SEPARATOR + item[\"metadata\"].get(\"name\", \"unknown\")\n",
    "\n",
    "                fasta_seq = item.get(\"extra_fields\", {}).get(\"sequence\", \"\")\n",
    "                fasta_string = fasta_header + \"\\n\" + fasta_seq\n",
    "                FASTA_STRINGS.append(fasta_string)\n",
    "\n",
    "        except error.HTTPError as e:\n",
    "            sys.stderr.write(\"Error processing URL: \" + str(next_url) + \"; Error: \" + str(e) + \"\\n\")\n",
    "            if e.code == 408 and attempts < 3:\n",
    "                attempts += 1\n",
    "                sleep(61)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        sleep(1) \n",
    "\n",
    "    return FASTA_STRINGS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function exports the individual fasta files to a given path\n",
    "#if you are using windows you must changes the \\ in the path to /\n",
    "#sorry about that, but thats windows baby\n",
    "\n",
    "def fasta_file_write(output_fasta_list,path):\n",
    "    fasta_list = output_fasta_list\n",
    "    pathvar = path\n",
    "    \n",
    "    for fasta in fasta_list:\n",
    "        fasta_string = fasta\n",
    "        fasta_string_2 = fasta\n",
    "        fasta_string_name , fasta_string_seq = fasta_string.strip('>').split('\\n')\n",
    "        print(fasta_string_name,fasta_string_seq,'\\n')\n",
    "        filename_list = fasta_string_2.strip('>').split('|')\n",
    "        filename = filename_list[0]\n",
    "        rec = SeqRecord(\n",
    "            Seq(fasta_string_seq),\n",
    "            id='',\n",
    "            name='',\n",
    "            description = fasta_string_name)\n",
    "        print('SeqRecord object : ',rec,sep='\\n')\n",
    "        records = [rec]\n",
    "        SeqIO.write(records,'{}/{}.fasta'.format(pathvar,filename),'fasta')\n",
    "        print('done with : ','{}/{}.fasta'.format(pathvar,filename))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7118eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_LOT = pd.read_pickle(\"/PATH/ncbi_2025_taxonomy_table.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = 'bgonzalez2412@gmail.com' \n",
    "\n",
    "\n",
    "def fetch_TAXID_data(tax_id, email):\n",
    "    classification = tax_id\n",
    "    Entrez.email = email  \n",
    "    retry_limit = 3  #number of tries i used\n",
    "\n",
    "    attempts = 0\n",
    "    while attempts < retry_limit:\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"taxonomy\", id=str(classification), retmode=\"xml\")\n",
    "            records = Entrez.read(handle)\n",
    "            for rec in records:\n",
    "                rank = rec.get(\"Rank\")\n",
    "            break  \n",
    "        except Exception as e:\n",
    "            print(f\"HTTP error: {str(e)} - Sleeping 10s and retrying for ID {classification}\")\n",
    "            sleep(10)\n",
    "            attempts += 1 \n",
    "        finally:\n",
    "            if 'handle' in locals() and handle:\n",
    "                handle.close()  #\n",
    "    if attempts == retry_limit:\n",
    "        print(f\"Failed after {retry_limit} retries for ID {classification}\")\n",
    "\n",
    "    return rank\n",
    "\n",
    "\n",
    "\n",
    "def fetch_LINEAGE_data(lineage_list, email):\n",
    "    Entrez.email = email  # Always set your email for NCBI's Entrez\n",
    "    lineage_df = {}\n",
    "    \n",
    "    for classification in lineage_list[2:]:  #im skipping the root section of the NCBI taxonome and cellular organisms\n",
    "        try:\n",
    "            rank_series = tax_LOT[tax_LOT['tax_id'] == classification]['rank']\n",
    "            if not rank_series.empty:\n",
    "                rank = rank_series.iloc[0]  # Use the first item from the series\n",
    "                if rank:\n",
    "                    lineage_df[rank] = classification\n",
    "                    print(f'Rank: {rank}, Tax ID: {classification}')\n",
    "                else:\n",
    "                    print(f'No rank available for Tax ID: {classification}')\n",
    "            else:\n",
    "                print(f'No entry found for Tax ID: {classification}, fetching from NCBI...')\n",
    "                rank = fetch_TAXID_data(classification, email)\n",
    "                if rank and rank != \"No rank available\":\n",
    "                    lineage_df[rank] = classification\n",
    "                else:\n",
    "                    print(f'Failed to fetch data for Tax ID: {classification} from NCBI.')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing Tax ID {classification}: {str(e)}')\n",
    "\n",
    "    return lineage_df\n",
    "\n",
    "# testing everything still works:\n",
    "list_tax = ['1', '0', '2624677', '1644055', '2157', '1236']\n",
    "email = 'your_email@example.com'  # Replace with your actual email\n",
    "print(fetch_LINEAGE_data(list_tax, email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e450fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function creates a list of dictionaries. The dictionaries are Key value pairs:\n",
    "# \"accession\" : A0X009XJT10 , \"IPR\" : IPR009908 , etc,....\n",
    "# I have set it up so that it pulls what I consider to be all the useful information contained in the requested JSON\n",
    "# dictionary from the interpro pull\n",
    "# if you want to modify it open the url in the browser to see the actual JSON contents or print them with your own code\n",
    "# then odify the for loop that contains the metadata section (its around \"metadata = {'Accession': accession,.....\")\n",
    "\n",
    "def fetch_data(BASE_URL, email):\n",
    "    context = ssl._create_unverified_context()\n",
    "    next_url = BASE_URL\n",
    "    data_list = []\n",
    "    bad_urls = []\n",
    "    \n",
    "    while next_url:\n",
    "        tries = 0  # Reset tries for each new URL\n",
    "        while tries <= 3: # I set limit to 3 worked fine \n",
    "            try:\n",
    "                if next_url in bad_urls:\n",
    "                    print(f\"Skipping known bad URL: {next_url}\")\n",
    "                    next_url = None\n",
    "                    break \n",
    "                    \n",
    "                req = request.Request(next_url, headers={\"Accept\": \"application/json\"})\n",
    "                res = request.urlopen(req, context=context)\n",
    "                \n",
    "                \n",
    "                tries = 0 \n",
    "                \n",
    "                if res.status == 204:\n",
    "                    print(\"Received 204. Ending fetch.\")\n",
    "                    next_url = None \n",
    "                    break\n",
    "\n",
    "                payload = json.loads(res.read().decode())\n",
    "                next_url = payload.get(\"next\", None)\n",
    "\n",
    "                for item in payload[\"results\"]:\n",
    "                    entries = None\n",
    "                    if \"entry_subset\" in item:\n",
    "                        entries = item[\"entry_subset\"]\n",
    "                    elif \"entries\" in item:\n",
    "                        entries = item[\"entries\"]\n",
    "                    \n",
    "                    if not entries:\n",
    "                        continue\n",
    "                    \n",
    "                    taxa = item['taxa']\n",
    "                    metadata = {\n",
    "                        'Accession': item[\"metadata\"][\"accession\"],\n",
    "                        'IPR': entries[0][\"accession\"],\n",
    "                        'name': item[\"metadata\"][\"name\"],\n",
    "                        'gene': item[\"metadata\"][\"gene\"],\n",
    "                        'protein_length': entries[0][\"protein_length\"],\n",
    "                        'in_alphafold': item[\"metadata\"][\"in_alphafold\"],\n",
    "                        'Source_Org_ID': item[\"metadata\"][\"source_organism\"][\"taxId\"],\n",
    "                        'Source_Org_Name': item[\"metadata\"][\"source_organism\"][\"scientificName\"],\n",
    "                        'taxa_range': len(taxa[0][\"lineage\"]),\n",
    "                        'Sequence': item[\"extra_fields\"][\"sequence\"],\n",
    "                    }\n",
    "\n",
    "                    lineage_df = fetch_LINEAGE_data(taxa[0]['lineage'], email)\n",
    "                    metadata.update(lineage_df)\n",
    "                    data_list.append(metadata)\n",
    "\n",
    "                sleep(1) # to not pull too fast\n",
    "                break \n",
    "\n",
    "            except HTTPError as e:\n",
    "                print(f\"HTTP Error {e.code} occurred for {next_url}. Try {tries+1} of 3.\")\n",
    "                if e.code == 408:\n",
    "                    sleep(61)\n",
    "                elif e.code == 404:\n",
    "                    sleep(61)\n",
    "                else:\n",
    "                    print(f\"Unhandled HTTP error {e.code}. Stopping fetch.\")\n",
    "                    next_url = None\n",
    "                    break \n",
    "                \n",
    "                tries += 1\n",
    "                if tries > 3:\n",
    "                    print(f\"Failed to fetch URL after {tries} tries: {next_url}. Marking as bad and moving on.\")\n",
    "                    bad_urls.append(next_url)\n",
    "                    next_url = None \n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "                next_url = None\n",
    "                break\n",
    "\n",
    "    return data_list, bad_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this simple function just creates a dataframe from the fetch_data list\n",
    "\n",
    "def create_dataframe(data_list):\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "def construct_csv(dataframe,pathvar,filename):\n",
    "    path = pathvar\n",
    "    name = filename\n",
    "    working_df = dataframe\n",
    "    header_df = list(working_df.keys())\n",
    "    \n",
    "    working_df.to_csv(path_or_buf=\"{}/{}.csv\".format(path,name),sep=\",\",header = header_df)\n",
    "    print(\"exported : \" + name + \" to : \" + path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294248c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#code used just for fastas\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = '/PATH/IPR012932'\n",
    "\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR012932/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# fasta stuff\n",
    "  email = 'your_email@example.com'  \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "\n",
    "  print(\"done VKOR\")\n",
    "    \n",
    "########################\n",
    "  pathvar = '/PATH/IPR009908'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR009908/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# fasta stuff\n",
    "  email = 'your_email@example.com'  \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "\n",
    "  print(\"done MauE\")\n",
    "    \n",
    "########################\n",
    "  pathvar = '/PATH/IPR003752'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR003752/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# fasta stuff\n",
    "  email = 'your_email@example.com'  \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "\n",
    "  print(\"done DsbB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25831f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPRs used: \n",
    "#    MauE: IPR009908\n",
    "#    VKOR: IPR012932\n",
    "#    DsbB: IPR003752\n",
    "#URLS used for archaea from interpro:\n",
    "#    VKOR: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR012932/taxonomy/uniprot/2157/?page_size=200&extra_fields=sequence\"\n",
    "#    mauE: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR009908/taxonomy/uniprot/2157/?page_size=200&extra_fields=sequence\"\n",
    "#    dsbB: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR003752/taxonomy/uniprot/2157/?page_size=200&extra_fields=sequence\"\n",
    "#URLS used for bacteria from interpro:\n",
    "#    VKOR: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR012932/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "#    MauE: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR009908/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "#    DsbB: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR003752/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code used for main data acquisition\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = '/PATH/IPR012932'\n",
    "\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR012932/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# fasta stuff\n",
    "  email = 'your_email@example.com'  \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "#exporting dataset to csv\n",
    "\n",
    "  archae_list = fetch_data(url,email)\n",
    "  archaea_df = create_dataframe(archaea_list)\n",
    "  name = \"bacteria_vkor_set\"\n",
    "  construct_csv(archaea_df,pathvar,name)\n",
    "\n",
    "  print(\"done VKOR\")\n",
    "    \n",
    "########################\n",
    "  pathvar = '/PATH/IPR009908'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR009908/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# fasta stuff\n",
    "  email = 'your_email@example.com'  \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "#exporting dataset to csv\n",
    "\n",
    "  archaea_list = fetch_data(url,email)\n",
    "  archaea_df = create_dataframe(archaea_list)\n",
    "  name = \"bacteria_maue_set\"\n",
    "  construct_csv(archaea_df,pathvar,name)\n",
    "\n",
    "  print(\"done MauE\")\n",
    "    \n",
    "########################\n",
    "  pathvar = '/PATH/IPR003752'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR003752/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# fasta stuff\n",
    "  email = 'your_email@example.com'  \n",
    "  fasta_list = output_fasta_list(url)\n",
    "  print(len(fasta_list))\n",
    "  fasta_file_write(fasta_list,pathvar)\n",
    "#exporting dataset to csv\n",
    "\n",
    "  archaea_list = fetch_data(url,email)\n",
    "  archaea_df = create_dataframe(archaea_list)\n",
    "  name = \"bacteria_dsbb_set\"\n",
    "  construct_csv(archaea_df,pathvar,name)\n",
    "\n",
    "  print(\"done DsbB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71005e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this sesction is for the control set it includes IPR\n",
    "# for ribsosomal_sub IPR047873: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR047873/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "# for RNA polymeras_sub IPR010243:  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR010243/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "# for signal peptidase IPR036286: \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR036286/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "# signal peptidase has far more entries, well see how that filters down into species counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code used for main data acquisition\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = '/PATH/IPR047873'\n",
    "\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR047873/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "\n",
    "#exporting dataset to csv\n",
    "  email = 'your_email@example.com'   \n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_RIBOSOME_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done Ribosomal subunit\")\n",
    "    \n",
    "########################\n",
    "  pathvar = '/PATH/IPR010243'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR010243/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "\n",
    "#exporting dataset to csv\n",
    "\n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_RIBOSOME_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done RNA poly subunit\")\n",
    "    \n",
    "########################\n",
    "  pathvar = 'C:/Users/bgonzale24/Desktop/MauE/notebooks/bacteria_set/IPR036286'\n",
    "   \n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR036286/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "\n",
    "#exporting dataset to csv\n",
    "  \n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_RIBOSOME_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done singnal peptidase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc569a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section is for MauG to determine its overlap with MauE\n",
    "#code used for main data acquisition\n",
    "#https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR026259/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = '/PATH/IPR026259'\n",
    "\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR026259/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "\n",
    "  email = 'your_email@example.com' \n",
    "#exporting dataset to csv\n",
    "\n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_MAUG_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done with MauG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08432d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section is for getting the Mau A data. this is the madh light chain. there are only 1199 proteins\n",
    "#https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR036560/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  pathvar = '/PATH/IPR036560'\n",
    "\n",
    "  url =  \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR036560/taxonomy/uniprot/2/?page_size=200&extra_fields=sequence\"\n",
    "  email = 'your_email@example.com' \n",
    "\n",
    "#exporting dataset to csv\n",
    "\n",
    "  bacteria_list = fetch_data(url,email)\n",
    "  bacteria_df = create_dataframe(bacteria_list)\n",
    "  name = \"bacteria_MAUA_set\"\n",
    "  construct_csv(bacteria_df,pathvar,name)\n",
    "\n",
    "  print(\"done with MauA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
