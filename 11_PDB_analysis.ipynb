{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aefd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "below are three functions, 1 gets a list of pdb ids that you derive by using the get_api function from the rcsb website.\n",
    "so you go there and do an advance search to get what you want then you click the get api button to generate an api json query.\n",
    "That is where the query variable I have here comes from.\n",
    "\n",
    "The others download the pdbs from that query which should be chunked for multiprocessing\n",
    "and the final one creates a dataframe of ncbi taxa info for the pdb ids for the purposes of analysis.\n",
    "\n",
    "I used this to search for bacterial pdb ids that have 1 or more dsbs (around 10035).\n",
    "I then got their taxa info which I use to cross against the MauE only catagories and the none catagories from the final set\n",
    "of species (from the 35 phylum listed in the figures and datatables 8 and 8b ).\n",
    "The pdbs in this list may then be downloaded as structures or their sequences retrieved for doing things like clustering or\n",
    "predicting topology via topcons. This meant things had to have a sequence entry which they did if they had a uniprot ID.\n",
    "\n",
    "I used topcons to remove things not exported to the periplasm, and I also removed entities without a proper uniprot ID. Many \n",
    "genes had multiple pdb entry structures that were redundant, so I examined them in the excell datatable \n",
    "\n",
    "These datatables are serperated into a folder to keep things sorta organized\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354487fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was the query used to get the PDB_ids for entry with 1 or more disulfide bonds and in bacteria\n",
    "#change 'false' to 'False' if remaking\n",
    "query = {\n",
    "  \"query\": {\n",
    "    \"type\": \"group\",\n",
    "    \"logical_operator\": \"and\",\n",
    "    \"nodes\": [\n",
    "      {\n",
    "        \"type\": \"terminal\",\n",
    "        \"service\": \"text\",\n",
    "        \"parameters\": {\n",
    "          \"attribute\": \"rcsb_entry_info.disulfide_bond_count\",\n",
    "          \"operator\": \"greater_or_equal\",\n",
    "          \"negation\": False, #this was changed to False from false and then the query worked \n",
    "          \"value\": 1\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"terminal\",\n",
    "        \"service\": \"text\",\n",
    "        \"parameters\": {\n",
    "          \"attribute\": \"rcsb_entity_source_organism.ncbi_parent_scientific_name\",\n",
    "          \"value\": \"Bacteria\",\n",
    "          \"operator\": \"exact_match\"\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"label\": \"text\"\n",
    "  },\n",
    "  \"return_type\": \"entry\",\n",
    "  \"request_options\": {\n",
    "    \"paginate\": {\n",
    "      \"start\": 0,\n",
    "      \"rows\": 25\n",
    "    },\n",
    "    \"results_content_type\": [\n",
    "      \"experimental\"\n",
    "    ],\n",
    "    \"sort\": [\n",
    "      {\n",
    "        \"sort_by\": \"score\",\n",
    "        \"direction\": \"desc\"\n",
    "      }\n",
    "    ],\n",
    "    \"scoring_strategy\": \"combined\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is really fast\n",
    "def pdb_id_getter(total_entries, query, divisor):\n",
    "    total = total_entries\n",
    "    chunks = divisor\n",
    "    chunk_size = total // chunks                     \n",
    "    num_remain = total - chunks * chunk_size  #this chunks the returned ids because there is a maxout I encountered      \n",
    "\n",
    "    url = \"https://search.rcsb.org/rcsbsearch/v2/query\"\n",
    "\n",
    "    data_total = []\n",
    "    for i in range(chunks):\n",
    "        start = i * chunk_size                       \n",
    "        q = dict(query)                              \n",
    "        q.setdefault(\"request_options\", {})\n",
    "        q[\"request_options\"][\"paginate\"] = {\"start\": start, \"rows\": chunk_size}\n",
    "\n",
    "        r = requests.post(url, json=q, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data_total.extend(r.json().get(\"result_set\", []))\n",
    "\n",
    "   \n",
    "    if num_remain > 0:\n",
    "        start = chunks * chunk_size                  \n",
    "        q = dict(query)\n",
    "        q.setdefault(\"request_options\", {})\n",
    "        q[\"request_options\"][\"paginate\"] = {\"start\": start, \"rows\": num_remain}\n",
    "\n",
    "        r = requests.post(url, json=q, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data_total.extend(r.json().get(\"result_set\", []))\n",
    "\n",
    "   \n",
    "    pdb_ids = [item[\"identifier\"] for item in data_total if \"identifier\" in item]\n",
    "    return pdb_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is really slow and should be done in parallel\n",
    "def pdb_downloader(out_path,pdb_ids):\n",
    "    pdb_list = pdb_ids\n",
    "    out = out_path\n",
    "    for pdb_id in pdb_ids:\n",
    "        pdb_url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "        response = requests.get(pdb_url)\n",
    "        with open(f\"{out}/{pdb_id}.pdb\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "            f.close()\n",
    "        print(f\"Downloaded {pdb_id}.pdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bellow is the parallel process stuff that makes the pdb dataset construction faster\n",
    "#the parallel funciton is general so you can run anything in this notebook as such\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c5b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids_test = '1A0D'\n",
    "\n",
    "def one_pdb_tax_uni_id_getter(pdb_ids, timeout = 60):\n",
    "    ret_df = pd.DataFrame()\n",
    "    ids_search = [pdb_ids]\n",
    "    time = timeout\n",
    "    taxids = []\n",
    "    uniprot_ids = []\n",
    "    sci_names = []\n",
    "    retry_limit = 3  \n",
    "\n",
    "    current = 0\n",
    "\n",
    "    # graphql query from the graphql data api tool from rcsb. I then copied that to here\n",
    "    query = \"\"\"\n",
    "        query annotations($id: String!) {\n",
    "          entry(entry_id: $id) {\n",
    "            polymer_entities {\n",
    "              rcsb_entity_source_organism {\n",
    "                ncbi_taxonomy_id\n",
    "                common_name\n",
    "                scientific_name\n",
    "                ncbi_scientific_name\n",
    "              }\n",
    "              uniprots {\n",
    "                rcsb_id\n",
    "                rcsb_uniprot_protein {\n",
    "                  source_organism {\n",
    "                    scientific_name\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    for i in ids_search:\n",
    "        attempts = 0\n",
    "        tax_id = None\n",
    "        sci_name = None\n",
    "        uniprot_id = None\n",
    "\n",
    "        while attempts < retry_limit:\n",
    "            try:\n",
    "                json1 = {\n",
    "                    \"query\": query,\n",
    "                    \"variables\": {\"id\": i}\n",
    "                }\n",
    "\n",
    "                r = requests.post(\"https://data.rcsb.org/graphql\", json=json1, timeout=time)\n",
    "                r.raise_for_status()\n",
    "                dict_temp = r.json()\n",
    "                #print(dict_temp)  \n",
    "\n",
    "        \n",
    "                if \"errors\" in dict_temp and dict_temp[\"errors\"]:\n",
    "                    raise RuntimeError(dict_temp[\"errors\"])\n",
    "\n",
    "                polys = dict_temp['data']['entry']['polymer_entities']\n",
    "\n",
    "                \n",
    "                found = False\n",
    "                for ent in polys:\n",
    "                    src_org = ent.get('rcsb_entity_source_organism', [])\n",
    "                    unis = ent.get('uniprots', [])\n",
    "                    if src_org and unis:\n",
    "                        tax_id = src_org[0].get('ncbi_taxonomy_id')\n",
    "                        sci_name = src_org[0].get('ncbi_scientific_name') or src_org[0].get('scientific_name')\n",
    "                        uniprot_id = unis[0].get('rcsb_id')\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "                if not found:\n",
    "                    raise ValueError(f\"No polymer entity with both taxonomy and UniProt for {i}\")\n",
    "\n",
    "                \n",
    "                taxids.append(tax_id)\n",
    "                sci_names.append(sci_name)\n",
    "                uniprot_ids.append(uniprot_id)\n",
    "                current += 1\n",
    "                #print(current) \n",
    "                break  \n",
    "\n",
    "            except Exception as e:\n",
    "                attempts += 1\n",
    "                print(f\"Error for {i} (attempt {attempts}/{retry_limit}): {e}\")\n",
    "                if attempts < retry_limit:\n",
    "                    sleep(10)\n",
    "\n",
    "        if tax_id is None and sci_name is None and uniprot_id is None:\n",
    "            taxids.append(None)\n",
    "            sci_names.append(None)\n",
    "            uniprot_ids.append(None)\n",
    "            current += 1\n",
    "            print(f\"{i} failed after {retry_limit} attempts\")\n",
    "            print(current)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"PDB_IDS\": ids_search[0],\n",
    "        \"UP_Accessions\": uniprot_id,\n",
    "        \"sci_names\": sci_name,\n",
    "        \"species\": tax_id,\n",
    "    }\n",
    "\n",
    "testdic = one_pdb_tax_uni_id_getter(ids_test, 60)\n",
    "print(testdic)\n",
    "print('----------\\n----------')\n",
    "\n",
    "###################\n",
    "\n",
    "def fetch_seq_phyl_for_one_pid(pid, tax_filter=None, sleep=0.0):\n",
    "    pid = pid\n",
    "    url = (\n",
    "        \"https://rest.uniprot.org/uniprotkb/search?\"\n",
    "        f\"query=(xref:pdb-{pid}%20OR%20xref:pdb-{pid})%20AND%20taxonomy_id:{tax_filter}\"\n",
    "        \"&fields=accession,xref_pdb,lineage_ids,sequence&format=tsv\"\n",
    "    ) \n",
    "    if sleep > 0:\n",
    "        time.sleep(sleep)\n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            break\n",
    "        except Exception:\n",
    "            if attempt == 2:\n",
    "                return {\n",
    "                    \"PDB_IDS\": pid,\n",
    "                    \"sequence\": 0,\n",
    "                    \"phylum\": 0,\n",
    "                }\n",
    "            time.sleep(1)\n",
    "\n",
    "    lines = r.text.strip().split(\"\\n\")\n",
    "\n",
    "    up_accessions = []\n",
    "    sequence_for_pid = 0\n",
    "    species_for_pid = 0\n",
    "\n",
    "    if len(lines) > 1:\n",
    "        for row in lines[1:]:\n",
    "            parsed_row = row.split(\"\\t\")\n",
    "            xref_str = parsed_row[1] #if you change the URL metadata order this will break. or if you add to it\n",
    "            lineage = parsed_row[2] #if you change the URL metadata order this will break. or if you add to it\n",
    "            lineages = lineage.split(',')\n",
    "            \n",
    "            for i in lineages:\n",
    "                if 'phylum' in i:\n",
    "                    phyl = i.split('(')[0] \n",
    "                    phylum = int(phyl)\n",
    "                    break\n",
    "                elif 'phylum' not in i:\n",
    "                    phylum = None\n",
    "                    #print('none_1')\n",
    "\n",
    "            for ref in xref_str.split(\";\"):\n",
    "                ref = ref.strip().upper()\n",
    "                \n",
    "                if ref == pid.upper():\n",
    "                    sequence_for_pid = parsed_row[3] #if you change the URL metadata order this will break. or if you add to it\n",
    "                    \n",
    "    elif len(lines) <= 1:\n",
    "        phylum = None\n",
    "        sequence_for_pid = None\n",
    "    return {\n",
    "        \"PDB_IDS\": pid,\n",
    "        \"sequence\": sequence_for_pid,\n",
    "        \"phylum\": phylum,\n",
    "    }\n",
    "\n",
    "testdic2 = fetch_seq_phyl_for_one_pid(ids_test, tax_filter=2, sleep=0.05)\n",
    "print(testdic2)\n",
    "print('----------\\n----------')\n",
    "\n",
    "###############################\n",
    "\n",
    "def run_parallel(func, items, max_workers=20, *args, **kwargs):\n",
    "    rows = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(func, item, *args, **kwargs): item\n",
    "            for item in items\n",
    "        }\n",
    "\n",
    "        for fut in as_completed(futures):\n",
    "            rows.append(fut.result())\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "'''\n",
    "These test the speed of the functions. it shoudl be somewhere around 2-3 pdb IDs a second depending on how you set the sleep\n",
    "times or how many threads you have available.\n",
    "'''\n",
    "start2 = time.time()\n",
    "test_parallel = ['8TT3', '8TTF', '8TTH', '8TUM', '8TUN', '8TUW', '8TUZ', '8TV3', '8TV8', '8TV9', '8TVA', '8TVB', '8TVD', '8TVJ', '8TW4', '8TW6', '8TXO', '8TYC', '8TYE', '8TYZ', '8TZQ', '8U0Q', '8U1Y', '8U49', '8U4A', '8U4F', '8U4V', '8U4X', '8U59', '8U5B', '8U65', '8U6X', '8U79', '8U7I', '8U8Z', '8UAV', '8UAW', '8UBQ', '8UEN', '8UF9', '8UHF', '8UIW', '8UO7', '8UP2', '8UPI', '8UPO', '8UPR', '8UQL', '8UQM', '8UQP', '8UR0', '8URA', '8URH', '8URI', '8URN', '8URO', '8URX', '8URY', '8USM', '8UT1', '8UTA', '8UTB', '8UTL', '8UUB', '8UUP', '8UVZ', '8UY6', '8V3J', '8V4W', '8V7P', '8V8W', '8V93', '8V9G', '8V9H', '8V9V', '8VAS', '8VBA', '8VBB', '8VE9', '8VEK', '8VEL', '8VEM']\n",
    "start = time.time()\n",
    "results = run_parallel(fetch_seq_phyl_for_one_pid,\n",
    "                       test_parallel,\n",
    "                       tax_filter=2,\n",
    "                       sleep=0.05,max_workers=50   \n",
    ")\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "\n",
    "print(f'testing seq fetcher, elapsed {elapsed:.2f} seconds, ~{len(test_parallel)/elapsed:.2f} ids/sec')\n",
    "print('----------\\n----------')\n",
    "\n",
    "\n",
    "test_parallel = ['8TT3', '8TTF', '8TTH', '8TUM', '8TUN', '8TUW', '8TUZ', '8TV3', '8TV8', '8TV9', '8TVA', '8TVB', '8TVD', '8TVJ', '8TW4', '8TW6', '8TXO', '8TYC', '8TYE', '8TYZ', '8TZQ', '8U0Q', '8U1Y', '8U49', '8U4A', '8U4F', '8U4V', '8U4X', '8U59', '8U5B', '8U65', '8U6X', '8U79', '8U7I', '8U8Z', '8UAV', '8UAW', '8UBQ', '8UEN', '8UF9', '8UHF', '8UIW', '8UO7', '8UP2', '8UPI', '8UPO', '8UPR', '8UQL', '8UQM', '8UQP', '8UR0', '8URA', '8URH', '8URI', '8URN', '8URO', '8URX', '8URY', '8USM', '8UT1', '8UTA', '8UTB', '8UTL', '8UUB', '8UUP', '8UVZ', '8UY6', '8V3J', '8V4W', '8V7P', '8V8W', '8V93', '8V9G', '8V9H', '8V9V', '8VAS', '8VBA', '8VBB', '8VE9', '8VEK', '8VEL', '8VEM']\n",
    "start = time.time()\n",
    "results2 = run_parallel(one_pdb_tax_uni_id_getter,\n",
    "                       test_parallel,\n",
    "                       timeout = 60,\n",
    "                       max_workers=50   \n",
    ")\n",
    "end = time.time()\n",
    "end2 = time.time()\n",
    "\n",
    "elapsed = end-start\n",
    "print(f'testing PDB meta fetcher, elapsed {elapsed:.2f} seconds, ~{len(test_parallel)/elapsed:.2f} ids/sec')\n",
    "\n",
    "test_df = pd.merge(results,results2, on='PDB_IDS', how='right' )\n",
    "test_df.to_excel(r\"C:\\PATH\\cleaned_functions_test.xlsx\")\n",
    "elapsed2 = end2-start2\n",
    "print(f'total time is {elapsed2}, with {len(test_parallel)/elapsed2} ids/sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1fc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_ids = pdb_id_getter(10035,query,10)\n",
    "print(pdb_ids,len(pdb_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the actual data acquisition\n",
    "start= time.time()\n",
    "results = run_parallel(fetch_seq_phyl_for_one_pid,\n",
    "                       pdb_ids,\n",
    "                       tax_filter=2,\n",
    "                       sleep=0.05,max_workers=50)\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "\n",
    "results2 = run_parallel(one_pdb_tax_uni_id_getter,\n",
    "                       pdb_ids,\n",
    "                       timeout = 60,\n",
    "                       max_workers=50)\n",
    "\n",
    "bact_dsbs_df = pd.merge(results,results2, on='PDB_IDS', how='right' )\n",
    "bact_dsbs_df.to_excel(r\"C:\\PATH\\10_bacterial_dsbs_wmetadata.xlsx\")\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print(f'exported total time is {elapsed}, with {len(test_parallel)/elapsed} ids/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312cbf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I drop whats not in the final phylogenetic analysis and remove redundant entries with the same sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bact_dsbs_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048871b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(bact_dsbs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"C:\\PATH\\5_Filtered_FBDS_IPRcounts_byspecies_MAUA_names.xlsx\")\n",
    "phyl = pd.read_excel(r\"C:\\PATH\\8b_MAUgenes_3cat_3mark_perc_no_can_10more_names.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f524a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "MauE_only = df[(df['IPR009908']>0) & (df['IPR003752'] == 0) & (df['IPR012932'] == 0)]\n",
    "None_only = df[(df['IPR009908']==0) & (df['IPR003752'] == 0) & (df['IPR012932'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MauE_only.shape,None_only.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be277ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "maue_none = pd.concat([MauE_only,None_only],ignore_index=True)\n",
    "print(maue_none.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maue_none['phylum'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9505487",
   "metadata": {},
   "outputs": [],
   "source": [
    "phyl_list = list(phyl['phylum'])\n",
    "for i in maue_none.index:\n",
    "    phylum = maue_none.at[i,'phylum']\n",
    "    if phylum not in phyl_list:\n",
    "        maue_none = maue_none.drop(i)\n",
    "print(maue_none.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maue_none['phylum'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this made a list of species that are in the final data analysis for filtering the pdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24263837",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = list(maue_none['species'])\n",
    "print(len(species_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaed63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_dsbs_df2 = bact_dsbs_df.copy()\n",
    "print(len(species_list))\n",
    "for i in bact_dsbs_df2.index:\n",
    "    taxa = bact_dsbs_df2.at[i,'species']\n",
    "    if taxa not in species_list:\n",
    "        bact_dsbs_df2 = bact_dsbs_df2.drop(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516af47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bact_dsbs_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bact_dsbs_df2['PDB_IDS'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd591947",
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_dsbs_df2.to_excel(r\"C:\\PATH\\11_bacterial_dsbs_wmetadata_filtered.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "now at this point I sorted the dataframe in excell by species ID. I did this because I wanted to see myself how many multiple\n",
    "entries there were for each species. At this point I saw how many redundant pdb entries there\n",
    "were for the same uniprot ID so I took the first pdb instance from the excel sheet (that was sorted by species ID in EXCEL not\n",
    "PANDAS !Important! because they sort tables differently. This was just my personal preference, I like the way excel sorts.)\n",
    "for each uniprot ID. So if there were 10 pdbs for P01552 I would only keep the PDB for the first one. The topcons information \n",
    "later comes from the full sequence from the uniprot ID entry (because it retains signal peptides and such) so the choice of PDB\n",
    "ID is arbitrary. But I am explaining this for reproducibility, and incase you want to repeat the data analysis on your own. If\n",
    "you are not careful you could get different PDB IDs from our work, likely representing the same uniprot IDs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_dsbs_df2 = pd.read_excel(r\"C:\\Users\\bgonzale24\\Desktop\\MauE\\notebooks\\notebooks_p1\\PDB_stuff\\11_bacterial_dsbs_wmetadata_filtered.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f93a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bact_dsbs_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_dsbs_df3 = bact_dsbs_df2.drop_duplicates(subset='UP_Accessions',keep='first')\n",
    "bact_dsbs_df3.to_excel(r\"C:\\PATH\\12_bacterial_dsbs_wmetadata_filtered_no_redun.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need to drop things without a sequence \n",
    "bact_dsbs_df3 = pd.read_excel(r\"C:\\PATH\\12_bacterial_dsbs_wmetadata_filtered_no_redun.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ec750",
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_dsbs_df3 =  bact_dsbs_df3.fillna(0)\n",
    "for i in bact_dsbs_df3.index:\n",
    "    sequence = bact_dsbs_df3.at[i,'sequence']\n",
    "    if sequence != 0:\n",
    "        print('seq')\n",
    "    elif sequence == 0:\n",
    "        print('noseq')\n",
    "        bact_dsbs_df3 = bact_dsbs_df3.drop(i)\n",
    "bact_dsbs_df3.to_excel(r\"C:\\PATH\\13_bacterial_dsbs_wmetadata_filtered_no_redun_wseqs.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I am going to export fastas for topcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bact_dsbs_df3.index:\n",
    "    acc = bact_dsbs_df3.at[i,'UP_Accessions']\n",
    "    sequence = bact_dsbs_df3.at[i,'sequence']\n",
    "    if sequence:\n",
    "        string = f'>{acc}\\n{sequence}\\n'\n",
    "        with open(r\"C:\\PATH\" + f'\\{acc}.fasta', 'w') as file:\n",
    "            file.write(string)\n",
    "            file.close()\n",
    "    elif not sequence:\n",
    "        print('noseq')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#at this point topcons was done using a docker conatiner as per the topcons2 github \n",
    "#I will now import the topcons data\n",
    "basedir = \"C:/PATH/\"\n",
    "topcons_results=[]\n",
    "for i in bact_dsbs_df3.index:\n",
    "    acc = bact_dsbs_df3.at[i,'UP_Accessions']\n",
    "    path = basedir + f\"{acc}/query.result.txt.fa\"\n",
    "    with open(path,'r') as file:\n",
    "        tc = file.readlines()[1]\n",
    "        topcons_results.append(tc)\n",
    "        file.close\n",
    "bact_dsbs_df3['TOPCONS2'] = topcons_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df249ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_dsbs_df3.to_excel(r\"C:\\PATH\\14_bacterial_dsbs_wmetadata_filtered_no_redun_wseqs_TC2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I drop stuff without a SP or TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfece98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping things without sp or TM\n",
    "print(bact_dsbs_df3.shape)\n",
    "for i in bact_dsbs_df3.index:\n",
    "    tc = bact_dsbs_df3.at[i,'TOPCONS2']\n",
    "    if 'S' in tc:\n",
    "        continue\n",
    "    elif 'M' in tc:\n",
    "        continue\n",
    "    else:\n",
    "        bact_dsbs_df3 = bact_dsbs_df3.drop(i)\n",
    "print(bact_dsbs_df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4af3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_dsbs_df3.to_excel(r\"C:\\PATH\\15_bacterial_dsbs_for_manual_check.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "At this point I can only manually annotate the dataset. This is because the disulfide bonds may still be junk that slipped \n",
    "through (Like artificial or mutated) or they may be functional like a CXXC or CXC active site. To the best of our out ability\n",
    "the group looked at the remaining 114 structures and annotated them as and pulled the genuine disulfide bond entries that \n",
    "we had a consensus on. These are found in data table 16. We then manually back checked if they were MauE only or None species\n",
    "and annotated that as well. This was finally used to make the table in the publication using word to make them nice and neat.\n",
    "There were a few duplicates that slipped by that I manually removed as well. \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
